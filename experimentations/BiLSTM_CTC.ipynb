{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "779ebf55",
   "metadata": {},
   "source": [
    "# 1. Pr√©paration des donn√©es et extraction des transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25d810b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de255ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les transcriptions\n",
    "df = pd.read_excel(\"C:/Users/Christian/Desktop/YembaTones/dataset2.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "582878a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WordId</th>\n",
       "      <th>Yemba</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>GroupeId</th>\n",
       "      <th>Statement</th>\n",
       "      <th>Syllabe 1</th>\n",
       "      <th>Tone 1</th>\n",
       "      <th>Syllabe 2</th>\n",
       "      <th>Tone 2</th>\n",
       "      <th>Syllabe 3</th>\n",
       "      <th>Tone 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3779</th>\n",
       "      <td>3780</td>\n",
       "      <td>lezeÃÑn</td>\n",
       "      <td>11</td>\n",
       "      <td>146</td>\n",
       "      <td>2</td>\n",
       "      <td>le</td>\n",
       "      <td>bas</td>\n",
       "      <td>zeÃÑn</td>\n",
       "      <td>moyen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3780</th>\n",
       "      <td>3781</td>\n",
       "      <td>nze≈ã</td>\n",
       "      <td>11</td>\n",
       "      <td>147</td>\n",
       "      <td>1</td>\n",
       "      <td>n</td>\n",
       "      <td>bas</td>\n",
       "      <td>ze≈ã</td>\n",
       "      <td>bas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3781</th>\n",
       "      <td>3782</td>\n",
       "      <td>nzeÃÅ≈ã</td>\n",
       "      <td>11</td>\n",
       "      <td>147</td>\n",
       "      <td>2</td>\n",
       "      <td>n</td>\n",
       "      <td>bas</td>\n",
       "      <td>zeÃÅ≈ã</td>\n",
       "      <td>haut</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3782</th>\n",
       "      <td>3783</td>\n",
       "      <td>nz…îÃÑ≈ã</td>\n",
       "      <td>11</td>\n",
       "      <td>148</td>\n",
       "      <td>1</td>\n",
       "      <td>n</td>\n",
       "      <td>bas</td>\n",
       "      <td>z…îÃÑ≈ã</td>\n",
       "      <td>moyen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3783</th>\n",
       "      <td>3784</td>\n",
       "      <td>nz…î≈ã</td>\n",
       "      <td>11</td>\n",
       "      <td>148</td>\n",
       "      <td>2</td>\n",
       "      <td>n</td>\n",
       "      <td>bas</td>\n",
       "      <td>z…î≈ã</td>\n",
       "      <td>bas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      WordId   Yemba  Speaker  GroupeId  Statement Syllabe 1 Tone 1 Syllabe 2  \\\n",
       "3779    3780  lezeÃÑn       11       146          2        le    bas      zeÃÑn   \n",
       "3780    3781    nze≈ã       11       147          1         n    bas       ze≈ã   \n",
       "3781    3782   nzeÃÅ≈ã       11       147          2         n    bas      zeÃÅ≈ã   \n",
       "3782    3783   nz…îÃÑ≈ã       11       148          1         n    bas      z…îÃÑ≈ã   \n",
       "3783    3784    nz…î≈ã       11       148          2         n    bas       z…î≈ã   \n",
       "\n",
       "     Tone 2 Syllabe 3 Tone 3  \n",
       "3779  moyen       NaN    NaN  \n",
       "3780    bas       NaN    NaN  \n",
       "3781   haut       NaN    NaN  \n",
       "3782  moyen       NaN    NaN  \n",
       "3783    bas       NaN    NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f8a6159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordId          0\n",
      "Yemba           0\n",
      "Speaker         0\n",
      "GroupeId        0\n",
      "Statement       0\n",
      "Syllabe 1       0\n",
      "Tone 1          0\n",
      "Syllabe 2      22\n",
      "Tone 2         22\n",
      "Syllabe 3    3762\n",
      "Tone 3       3762\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8713fdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remplacement des valeurs manquates\n",
    "df[\"Syllabe 2\"] = df[\"Syllabe 2\"].fillna(\"‚àÖ\")\n",
    "df[\"Tone 2\"]    = df[\"Tone 2\"].fillna(\"‚àÖ\")\n",
    "df[\"Syllabe 3\"] = df[\"Syllabe 3\"].fillna(\"‚àÖ\")\n",
    "df[\"Tone 3\"]    = df[\"Tone 3\"].fillna(\"‚àÖ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69570c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# G√©n√®re les chemins des fichiers audio associ√©s\n",
    "def get_audio_path(row):\n",
    "    return f\"C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_{row['Speaker']}/group_{row['GroupeId']}/spkr_{row['Speaker']}_group_{row['GroupeId']}_statement_{int(row['Statement'])}.wav\"\n",
    "\n",
    "df[\"audio_path\"] = df.apply(get_audio_path, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ec054be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WordId</th>\n",
       "      <th>Yemba</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>GroupeId</th>\n",
       "      <th>Statement</th>\n",
       "      <th>Syllabe 1</th>\n",
       "      <th>Tone 1</th>\n",
       "      <th>Syllabe 2</th>\n",
       "      <th>Tone 2</th>\n",
       "      <th>Syllabe 3</th>\n",
       "      <th>Tone 3</th>\n",
       "      <th>audio_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3779</th>\n",
       "      <td>3780</td>\n",
       "      <td>lezeÃÑn</td>\n",
       "      <td>11</td>\n",
       "      <td>146</td>\n",
       "      <td>2</td>\n",
       "      <td>le</td>\n",
       "      <td>bas</td>\n",
       "      <td>zeÃÑn</td>\n",
       "      <td>moyen</td>\n",
       "      <td>‚àÖ</td>\n",
       "      <td>‚àÖ</td>\n",
       "      <td>C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_11/group_146/spkr_11_group_146_statement_2.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3780</th>\n",
       "      <td>3781</td>\n",
       "      <td>nze≈ã</td>\n",
       "      <td>11</td>\n",
       "      <td>147</td>\n",
       "      <td>1</td>\n",
       "      <td>n</td>\n",
       "      <td>bas</td>\n",
       "      <td>ze≈ã</td>\n",
       "      <td>bas</td>\n",
       "      <td>‚àÖ</td>\n",
       "      <td>‚àÖ</td>\n",
       "      <td>C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_11/group_147/spkr_11_group_147_statement_1.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3781</th>\n",
       "      <td>3782</td>\n",
       "      <td>nzeÃÅ≈ã</td>\n",
       "      <td>11</td>\n",
       "      <td>147</td>\n",
       "      <td>2</td>\n",
       "      <td>n</td>\n",
       "      <td>bas</td>\n",
       "      <td>zeÃÅ≈ã</td>\n",
       "      <td>haut</td>\n",
       "      <td>‚àÖ</td>\n",
       "      <td>‚àÖ</td>\n",
       "      <td>C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_11/group_147/spkr_11_group_147_statement_2.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3782</th>\n",
       "      <td>3783</td>\n",
       "      <td>nz…îÃÑ≈ã</td>\n",
       "      <td>11</td>\n",
       "      <td>148</td>\n",
       "      <td>1</td>\n",
       "      <td>n</td>\n",
       "      <td>bas</td>\n",
       "      <td>z…îÃÑ≈ã</td>\n",
       "      <td>moyen</td>\n",
       "      <td>‚àÖ</td>\n",
       "      <td>‚àÖ</td>\n",
       "      <td>C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_11/group_148/spkr_11_group_148_statement_1.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3783</th>\n",
       "      <td>3784</td>\n",
       "      <td>nz…î≈ã</td>\n",
       "      <td>11</td>\n",
       "      <td>148</td>\n",
       "      <td>2</td>\n",
       "      <td>n</td>\n",
       "      <td>bas</td>\n",
       "      <td>z…î≈ã</td>\n",
       "      <td>bas</td>\n",
       "      <td>‚àÖ</td>\n",
       "      <td>‚àÖ</td>\n",
       "      <td>C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_11/group_148/spkr_11_group_148_statement_2.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      WordId   Yemba  Speaker  GroupeId  Statement Syllabe 1 Tone 1 Syllabe 2  \\\n",
       "3779    3780  lezeÃÑn       11       146          2        le    bas      zeÃÑn   \n",
       "3780    3781    nze≈ã       11       147          1         n    bas       ze≈ã   \n",
       "3781    3782   nzeÃÅ≈ã       11       147          2         n    bas      zeÃÅ≈ã   \n",
       "3782    3783   nz…îÃÑ≈ã       11       148          1         n    bas      z…îÃÑ≈ã   \n",
       "3783    3784    nz…î≈ã       11       148          2         n    bas       z…î≈ã   \n",
       "\n",
       "     Tone 2 Syllabe 3 Tone 3  \\\n",
       "3779  moyen         ‚àÖ      ‚àÖ   \n",
       "3780    bas         ‚àÖ      ‚àÖ   \n",
       "3781   haut         ‚àÖ      ‚àÖ   \n",
       "3782  moyen         ‚àÖ      ‚àÖ   \n",
       "3783    bas         ‚àÖ      ‚àÖ   \n",
       "\n",
       "                                                                                                                                                                                                   audio_path  \n",
       "3779  C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_11/group_146/spkr_11_group_146_statement_2.wav  \n",
       "3780  C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_11/group_147/spkr_11_group_147_statement_1.wav  \n",
       "3781  C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_11/group_147/spkr_11_group_147_statement_2.wav  \n",
       "3782  C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_11/group_148/spkr_11_group_148_statement_1.wav  \n",
       "3783  C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_11/group_148/spkr_11_group_148_statement_2.wav  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a87a71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combiner les syllabes avec tons pour cr√©er une transcription syllabique\n",
    "def combine_syllables(row):\n",
    "    syllables = []\n",
    "    for i in range(1, 4):\n",
    "        syll = row.get(f\"Syllabe {i}\")\n",
    "        tone = row.get(f\"Tone {i}\")\n",
    "        if pd.notnull(syll) and pd.notnull(tone):\n",
    "            syllables.append(f\"{syll}|{tone}\")\n",
    "    return \" \".join(syllables)\n",
    "\n",
    "df[\"syllable_transcript\"] = df.apply(combine_syllables, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df2aaabd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WordId</th>\n",
       "      <th>Yemba</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>GroupeId</th>\n",
       "      <th>Statement</th>\n",
       "      <th>Syllabe 1</th>\n",
       "      <th>Tone 1</th>\n",
       "      <th>Syllabe 2</th>\n",
       "      <th>Tone 2</th>\n",
       "      <th>Syllabe 3</th>\n",
       "      <th>Tone 3</th>\n",
       "      <th>audio_path</th>\n",
       "      <th>syllable_transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Apa</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>bas</td>\n",
       "      <td>pa</td>\n",
       "      <td>bas</td>\n",
       "      <td>‚àÖ</td>\n",
       "      <td>‚àÖ</td>\n",
       "      <td>C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_1/group_1/spkr_1_group_1_statement_1.wav</td>\n",
       "      <td>a|bas pa|bas ‚àÖ|‚àÖ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>ApaÃÑ</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>a</td>\n",
       "      <td>bas</td>\n",
       "      <td>paÃÑ</td>\n",
       "      <td>moyen</td>\n",
       "      <td>‚àÖ</td>\n",
       "      <td>‚àÖ</td>\n",
       "      <td>C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_1/group_1/spkr_1_group_1_statement_2.wav</td>\n",
       "      <td>a|bas paÃÑ|moyen ‚àÖ|‚àÖ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>ApaÃÅ</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>a</td>\n",
       "      <td>bas</td>\n",
       "      <td>paÃÅ</td>\n",
       "      <td>haut</td>\n",
       "      <td>‚àÖ</td>\n",
       "      <td>‚àÖ</td>\n",
       "      <td>C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_1/group_1/spkr_1_group_1_statement_3.wav</td>\n",
       "      <td>a|bas paÃÅ|haut ‚àÖ|‚àÖ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Api</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>bas</td>\n",
       "      <td>pi</td>\n",
       "      <td>bas</td>\n",
       "      <td>‚àÖ</td>\n",
       "      <td>‚àÖ</td>\n",
       "      <td>C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_1/group_2/spkr_1_group_2_statement_1.wav</td>\n",
       "      <td>a|bas pi|bas ‚àÖ|‚àÖ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>ApiÃÑ</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>a</td>\n",
       "      <td>bas</td>\n",
       "      <td>piÃÑ</td>\n",
       "      <td>moyen</td>\n",
       "      <td>‚àÖ</td>\n",
       "      <td>‚àÖ</td>\n",
       "      <td>C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_1/group_2/spkr_1_group_2_statement_2.wav</td>\n",
       "      <td>a|bas piÃÑ|moyen  ‚àÖ|‚àÖ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   WordId Yemba  Speaker  GroupeId  Statement Syllabe 1 Tone 1 Syllabe 2  \\\n",
       "0       1   Apa        1         1          1         a    bas        pa   \n",
       "1       2  ApaÃÑ        1         1          2         a    bas       paÃÑ   \n",
       "2       3  ApaÃÅ        1         1          3         a    bas       paÃÅ   \n",
       "3       4   Api        1         2          1         a    bas        pi   \n",
       "4       5  ApiÃÑ        1         2          2         a    bas       piÃÑ   \n",
       "\n",
       "   Tone 2 Syllabe 3 Tone 3  \\\n",
       "0     bas         ‚àÖ      ‚àÖ   \n",
       "1   moyen         ‚àÖ      ‚àÖ   \n",
       "2    haut         ‚àÖ      ‚àÖ   \n",
       "3     bas         ‚àÖ      ‚àÖ   \n",
       "4  moyen          ‚àÖ      ‚àÖ   \n",
       "\n",
       "                                                                                                                                                                                          audio_path  \\\n",
       "0  C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_1/group_1/spkr_1_group_1_statement_1.wav   \n",
       "1  C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_1/group_1/spkr_1_group_1_statement_2.wav   \n",
       "2  C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_1/group_1/spkr_1_group_1_statement_3.wav   \n",
       "3  C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_1/group_2/spkr_1_group_2_statement_1.wav   \n",
       "4  C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_1/group_2/spkr_1_group_2_statement_2.wav   \n",
       "\n",
       "    syllable_transcript  \n",
       "0      a|bas pa|bas ‚àÖ|‚àÖ  \n",
       "1   a|bas paÃÑ|moyen ‚àÖ|‚àÖ  \n",
       "2    a|bas paÃÅ|haut ‚àÖ|‚àÖ  \n",
       "3      a|bas pi|bas ‚àÖ|‚àÖ  \n",
       "4  a|bas piÃÑ|moyen  ‚àÖ|‚àÖ  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c010adaf",
   "metadata": {},
   "source": [
    "# 2. Extraction des caract√©ristiques audio avec MelSpectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90d93d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "\n",
    "def extract_melspectrogram(file_path, sample_rate=16000, n_mels=80):\n",
    "    try:\n",
    "        waveform, sr = torchaudio.load(file_path)\n",
    "\n",
    "        # Force mono (si 2 canaux, on moyenne)\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "        # Resample si n√©cessaire\n",
    "        if sr != sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "\n",
    "        # Transformer Mel spectrogramme\n",
    "        mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=1024,\n",
    "            hop_length=256,\n",
    "            n_mels=n_mels\n",
    "        )\n",
    "        mel_spec = mel_transform(waveform)\n",
    "\n",
    "        # Convertir en dB\n",
    "        mel_spec = torchaudio.transforms.AmplitudeToDB()(mel_spec)\n",
    "\n",
    "        # [1, F, T] ‚Üí [T, F]\n",
    "        mel_spec = mel_spec.squeeze(0).transpose(0, 1)\n",
    "\n",
    "        # V√©rifier la forme finale\n",
    "        if mel_spec.shape[1] != n_mels:\n",
    "            raise ValueError(f\"Mel spectrogram with invalid feature size: {mel_spec.shape}\")\n",
    "\n",
    "        return mel_spec\n",
    "\n",
    "    except Exception as e:\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6644c68a",
   "metadata": {},
   "source": [
    "# 3. Tokenisation syllabique et construction du vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d368113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Cr√©er un vocabulaire syllabique\n",
    "syllables = df[\"syllable_transcript\"].str.split().sum()\n",
    "syllable_counts = Counter(syllables)\n",
    "vocab = {s: i + 1 for i, s in enumerate(sorted(syllable_counts))}\n",
    "vocab[\"<BLANK>\"] = 0  # pour CTC\n",
    "\n",
    "# Encodage des transcriptions\n",
    "def encode_transcript(syllable_transcript):\n",
    "    return [vocab[s] for s in syllable_transcript.split()]\n",
    "\n",
    "df[\"encoded\"] = df[\"syllable_transcript\"].map(encode_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e4da4cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WordId</th>\n",
       "      <th>Yemba</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>GroupeId</th>\n",
       "      <th>Statement</th>\n",
       "      <th>Syllabe 1</th>\n",
       "      <th>Tone 1</th>\n",
       "      <th>Syllabe 2</th>\n",
       "      <th>Tone 2</th>\n",
       "      <th>Syllabe 3</th>\n",
       "      <th>Tone 3</th>\n",
       "      <th>audio_path</th>\n",
       "      <th>syllable_transcript</th>\n",
       "      <th>encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Apa</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>bas</td>\n",
       "      <td>pa</td>\n",
       "      <td>bas</td>\n",
       "      <td>‚àÖ</td>\n",
       "      <td>‚àÖ</td>\n",
       "      <td>C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_1/group_1/spkr_1_group_1_statement_1.wav</td>\n",
       "      <td>a|bas pa|bas ‚àÖ|‚àÖ</td>\n",
       "      <td>[11, 200, 314]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>ApaÃÑ</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>a</td>\n",
       "      <td>bas</td>\n",
       "      <td>paÃÑ</td>\n",
       "      <td>moyen</td>\n",
       "      <td>‚àÖ</td>\n",
       "      <td>‚àÖ</td>\n",
       "      <td>C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_1/group_1/spkr_1_group_1_statement_2.wav</td>\n",
       "      <td>a|bas paÃÑ|moyen ‚àÖ|‚àÖ</td>\n",
       "      <td>[11, 203, 314]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>ApaÃÅ</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>a</td>\n",
       "      <td>bas</td>\n",
       "      <td>paÃÅ</td>\n",
       "      <td>haut</td>\n",
       "      <td>‚àÖ</td>\n",
       "      <td>‚àÖ</td>\n",
       "      <td>C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_1/group_1/spkr_1_group_1_statement_3.wav</td>\n",
       "      <td>a|bas paÃÅ|haut ‚àÖ|‚àÖ</td>\n",
       "      <td>[11, 201, 314]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Api</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>bas</td>\n",
       "      <td>pi</td>\n",
       "      <td>bas</td>\n",
       "      <td>‚àÖ</td>\n",
       "      <td>‚àÖ</td>\n",
       "      <td>C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_1/group_2/spkr_1_group_2_statement_1.wav</td>\n",
       "      <td>a|bas pi|bas ‚àÖ|‚àÖ</td>\n",
       "      <td>[11, 209, 314]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>ApiÃÑ</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>a</td>\n",
       "      <td>bas</td>\n",
       "      <td>piÃÑ</td>\n",
       "      <td>moyen</td>\n",
       "      <td>‚àÖ</td>\n",
       "      <td>‚àÖ</td>\n",
       "      <td>C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_1/group_2/spkr_1_group_2_statement_2.wav</td>\n",
       "      <td>a|bas piÃÑ|moyen  ‚àÖ|‚àÖ</td>\n",
       "      <td>[11, 212, 314]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   WordId Yemba  Speaker  GroupeId  Statement Syllabe 1 Tone 1 Syllabe 2  \\\n",
       "0       1   Apa        1         1          1         a    bas        pa   \n",
       "1       2  ApaÃÑ        1         1          2         a    bas       paÃÑ   \n",
       "2       3  ApaÃÅ        1         1          3         a    bas       paÃÅ   \n",
       "3       4   Api        1         2          1         a    bas        pi   \n",
       "4       5  ApiÃÑ        1         2          2         a    bas       piÃÑ   \n",
       "\n",
       "   Tone 2 Syllabe 3 Tone 3  \\\n",
       "0     bas         ‚àÖ      ‚àÖ   \n",
       "1   moyen         ‚àÖ      ‚àÖ   \n",
       "2    haut         ‚àÖ      ‚àÖ   \n",
       "3     bas         ‚àÖ      ‚àÖ   \n",
       "4  moyen          ‚àÖ      ‚àÖ   \n",
       "\n",
       "                                                                                                                                                                                          audio_path  \\\n",
       "0  C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_1/group_1/spkr_1_group_1_statement_1.wav   \n",
       "1  C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_1/group_1/spkr_1_group_1_statement_2.wav   \n",
       "2  C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_1/group_1/spkr_1_group_1_statement_3.wav   \n",
       "3  C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_1/group_2/spkr_1_group_2_statement_1.wav   \n",
       "4  C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_1/group_2/spkr_1_group_2_statement_2.wav   \n",
       "\n",
       "    syllable_transcript         encoded  \n",
       "0      a|bas pa|bas ‚àÖ|‚àÖ  [11, 200, 314]  \n",
       "1   a|bas paÃÑ|moyen ‚àÖ|‚àÖ  [11, 203, 314]  \n",
       "2    a|bas paÃÅ|haut ‚àÖ|‚àÖ  [11, 201, 314]  \n",
       "3      a|bas pi|bas ‚àÖ|‚àÖ  [11, 209, 314]  \n",
       "4  a|bas piÃÑ|moyen  ‚àÖ|‚àÖ  [11, 212, 314]  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2d91d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3779    C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_11/group_146/spkr_11_group_146_statement_2.wav\n",
       "3780    C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_11/group_147/spkr_11_group_147_statement_1.wav\n",
       "3781    C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_11/group_147/spkr_11_group_147_statement_2.wav\n",
       "3782    C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_11/group_148/spkr_11_group_148_statement_1.wav\n",
       "3783    C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_11/group_148/spkr_11_group_148_statement_2.wav\n",
       "Name: audio_path, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"audio_path\"].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27896f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichiers manquants : 0\n",
      "Empty DataFrame\n",
      "Columns: [audio_path, Speaker, GroupeId, Statement]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "df[\"exists\"] = df[\"audio_path\"].apply(lambda p: os.path.exists(p))\n",
    "df = df[df[\"exists\"]]\n",
    "\n",
    "missing = df[~df[\"exists\"]]\n",
    "print(\"Fichiers manquants :\", len(missing))\n",
    "print(missing[[\"audio_path\", \"Speaker\", \"GroupeId\", \"Statement\"]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2660925",
   "metadata": {},
   "source": [
    "# 4. Split en train/valid/test (80/10/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eaf98c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2684 train\n",
      "335 val\n",
      "336 test\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val, test = train_test_split(test, test_size=0.5, random_state=42)\n",
    "\n",
    "print(len(train), \"train\")\n",
    "print(len(val), \"val\")\n",
    "print(len(test), \"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3790bd",
   "metadata": {},
   "source": [
    "# 5. Dataset & DataLoader PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e8b5e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class YembaDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        try:\n",
    "            mel = extract_melspectrogram(row[\"audio_path\"])\n",
    "            target = torch.tensor(row[\"encoded\"], dtype=torch.long)\n",
    "            return mel, target\n",
    "        except Exception as e:\n",
    "            return None "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8fa545",
   "metadata": {},
   "source": [
    "# 6. Collate function pour padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c9ce050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Supprimer les entr√©es incorrectes\n",
    "    batch = [b for b in batch if b is not None and b[0] is not None and b[1] is not None]\n",
    "\n",
    "    if len(batch) == 0:\n",
    "        return None\n",
    "\n",
    "    inputs, targets = zip(*batch)\n",
    "    input_lengths = [i.shape[0] for i in inputs]\n",
    "    target_lengths = [t.shape[0] for t in targets]\n",
    "\n",
    "    inputs_padded = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True)\n",
    "    targets_cat = torch.cat(targets)\n",
    "\n",
    "    return inputs_padded, targets_cat, input_lengths, target_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f3bc329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Cr√©ation des datasets\n",
    "train_dataset = YembaDataset(train)\n",
    "val_dataset = YembaDataset(val)\n",
    "test_dataset = YembaDataset(test)\n",
    "\n",
    "# Cr√©ation des DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24af611e",
   "metadata": {},
   "source": [
    "# 7. Mod√®le BiLSTM + CTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "178cb07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BiLSTM_CTC(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, vocab_size, num_layers=3, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers,\n",
    "                            batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.classifier = nn.Linear(hidden_dim * 2, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "008d3727",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = BiLSTM_CTC(input_dim=80, hidden_dim=256, vocab_size=len(vocab)).to(device)\n",
    "criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Vocab inverse pour d√©codage\n",
    "vocab_inv = {v: k for k, v in vocab.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbba170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"vocab1.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(vocab, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1507ef",
   "metadata": {},
   "source": [
    "# 8. Entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07ec9094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CTCLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = BiLSTM_CTC(input_dim=80, hidden_dim=256, vocab_size=len(vocab))\n",
    "criterion = CTCLoss(blank=0, zero_infinity=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "train_loader = DataLoader(YembaDataset(train), batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(YembaDataset(val), batch_size=8, collate_fn=collate_fn)\n",
    "\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for mel, target, input_lens, target_lens in tqdm(train_loader):\n",
    "        logit = model(mel)\n",
    "        logit = logit.log_softmax(2).transpose(0, 1)  # [T, B, C]\n",
    "        \n",
    "        output_lengths = model.compute_output_lengths(torch.tensor(input_lens)).to(device)\n",
    "        target_lens = torch.tensor(target_lens, dtype=torch.long).to(device)\n",
    "        \n",
    "        loss = criterion(logit, target, input_lens, target_lens)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb6616d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- √âpoque 1 ---\n",
      "Perte entra√Ænement : 5.5702\n",
      "Perte validation : 3.4317\n",
      "WER entra√Ænement : 16.8366\n",
      "WER validation   : 16.4750\n",
      "Temps d'entra√Ænement : 578.23 secondes\n",
      "Nouveau meilleur mod√®le sauvegard√©\n",
      "\n",
      "--- √âpoque 2 ---\n",
      "Perte entra√Ænement : 3.7592\n",
      "Perte validation : 3.3272\n",
      "WER entra√Ænement : 16.8373\n",
      "WER validation   : 16.4750\n",
      "Temps d'entra√Ænement : 771.80 secondes\n",
      "Nouveau meilleur mod√®le sauvegard√©\n",
      "\n",
      "--- √âpoque 3 ---\n",
      "Perte entra√Ænement : 3.6741\n",
      "Perte validation : 3.6083\n",
      "WER entra√Ænement : 16.8743\n",
      "WER validation   : 16.4882\n",
      "Temps d'entra√Ænement : 869.45 secondes\n",
      "\n",
      "--- √âpoque 4 ---\n",
      "Perte entra√Ænement : 3.7298\n",
      "Perte validation : 3.2955\n",
      "WER entra√Ænement : 16.8547\n",
      "WER validation   : 16.4892\n",
      "Temps d'entra√Ænement : 755.24 secondes\n",
      "Nouveau meilleur mod√®le sauvegard√©\n",
      "\n",
      "--- √âpoque 5 ---\n",
      "Perte entra√Ænement : 3.7923\n",
      "Perte validation : 3.3290\n"
     ]
    }
   ],
   "source": [
    "import jiwer\n",
    "import time\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "patience_counter = 0\n",
    "num_epochs = 30\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "wer_train = []\n",
    "wer_val = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\n--- √âpoque {epoch + 1} ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Mode entra√Ænement\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for mel, target, input_lens, target_lens in train_loader:\n",
    "        mel, target = mel.to(device), target.to(device)\n",
    "\n",
    "        logits = model(mel)\n",
    "        logits = logits.log_softmax(2).transpose(0, 1)  # [T, B, C]\n",
    "\n",
    "        # recalcul des input_lengths apr√®s les CNN\n",
    "        output_lengths = torch.tensor(input_lens, dtype=torch.long)\n",
    "\n",
    "        loss = criterion(\n",
    "            logits,\n",
    "            target,\n",
    "            output_lengths.to(device),                 # Tenseur Long\n",
    "            torch.tensor(target_lens, dtype=torch.long).to(device)  # Conversion explicite\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Perte entra√Ænement : {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Mode validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for mel, target, input_lens, target_lens in val_loader:\n",
    "            mel, target = mel.to(device), target.to(device)\n",
    "            logits = model(mel)\n",
    "            logits = logits.log_softmax(2).transpose(0, 1)\n",
    "\n",
    "            output_lengths = torch.tensor(input_lens, dtype=torch.long)\n",
    "            target_lens = torch.tensor(target_lens, dtype=torch.long).to(device)\n",
    "            loss = criterion(logits, target, output_lengths, target_lens)\n",
    "\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Perte validation : {val_loss:.4f}\")\n",
    "\n",
    "    # Stocker les pertes et WER √† chaque √©poque\n",
    "    train_losses.append(total_loss / len(train_loader))\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # √âvaluer WER train & val (simplifi√©)\n",
    "    def quick_wer(loader):\n",
    "        model.eval()\n",
    "        predictions, references = [], []\n",
    "        with torch.no_grad():\n",
    "            for mel, target, input_lens, target_lens in loader:\n",
    "                mel = mel.to(device)\n",
    "                out = model(mel).log_softmax(2)\n",
    "                pred = out.argmax(2)\n",
    "                for i in range(len(mel)):\n",
    "                    pred_seq = pred[i][:input_lens[i]].tolist()\n",
    "                    ref_seq = target[:target_lens[i]]\n",
    "                    predictions.append(\" \".join([vocab_inv.get(p, \"\") for p in pred_seq if p != 0]))\n",
    "                    references.append(\" \".join([vocab_inv.get(t.item(), \"\") for t in ref_seq]))\n",
    "        return jiwer.wer(references, predictions)\n",
    "\n",
    "    # √âvaluer WER et afficher les r√©sultats\n",
    "    wer_t = quick_wer(train_loader)\n",
    "    wer_v = quick_wer(val_loader)\n",
    "    wer_train.append(wer_t)\n",
    "    wer_val.append(wer_v)\n",
    "\n",
    "    print(f\"WER entra√Ænement : {wer_t:.4f}\")\n",
    "    print(f\"WER validation   : {wer_v:.4f}\")\n",
    "    end_time = time.time()\n",
    "    print(f\"Temps d'entra√Ænement : {(end_time - start_time):.2f} secondes\")\n",
    "        \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"best_model1.pt\")\n",
    "        print(\"Nouveau meilleur mod√®le sauvegard√©\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Arr√™t anticip√© (early stopping)\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61f9d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "epochs = list(range(1, len(train_losses) + 1))\n",
    "\n",
    "# üìâ Courbe des pertes (Loss)\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_losses, label='Perte Entra√Ænement')\n",
    "plt.plot(epochs, val_losses, label='Perte Validation')\n",
    "plt.title(\"√âvolution de la Perte\")\n",
    "plt.xlabel(\"√âpoque\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# üß† Courbe du Word Error Rate (WER)\n",
    "plt.figure()\n",
    "plt.plot(epochs, wer_train, label='WER Entra√Ænement')\n",
    "plt.plot(epochs, wer_val, label='WER Validation')\n",
    "plt.title(\"√âvolution du WER (Word Error Rate)\")\n",
    "plt.xlabel(\"√âpoque\")\n",
    "plt.ylabel(\"WER\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff29d1d",
   "metadata": {},
   "source": [
    "# 9.evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a01f982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader, vocab_inv):\n",
    "    model.eval()\n",
    "    predictions, references = [], []\n",
    "    with torch.no_grad():\n",
    "        for mel, target, input_lens, target_lens in loader:\n",
    "            out = model(mel).log_softmax(2)\n",
    "            pred = out.argmax(2)\n",
    "            for i in range(len(mel)):\n",
    "                pred_seq = pred[i][:input_lens[i]].tolist()\n",
    "                decoded = [vocab_inv[p] for p in pred_seq if p in vocab_inv and p != 0]\n",
    "                ref = [vocab_inv[t.item()] for t in target[:target_lens[i]]]\n",
    "                predictions.append(\" \".join(decoded))\n",
    "                references.append(\" \".join(ref))\n",
    "\n",
    "    wer = jiwer.wer(references, predictions)\n",
    "    cer = jiwer.cer(references, predictions)\n",
    "    ser = sum([r != p for r, p in zip(references, predictions)]) / len(references)\n",
    "    return wer, cer, ser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0f961e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiwer import wer, cer\n",
    "\n",
    "# Charger le meilleur mod√®le\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_refs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for mel, target, input_lens, target_lens in test_loader:\n",
    "        mel = mel.to(device)\n",
    "        output = model(mel).log_softmax(2)\n",
    "        pred = output.argmax(2)  # [B, T]\n",
    "        pred = pred.cpu()\n",
    "\n",
    "        for i in range(len(mel)):\n",
    "            pred_seq = pred[i][:input_lens[i]].tolist()\n",
    "            decoded_pred = [vocab_inv.get(p, \"\") for p in pred_seq if p != 0]\n",
    "            ref_seq = target[:target_lens[i]]\n",
    "            decoded_ref = [vocab_inv.get(t.item(), \"\") for t in ref_seq]\n",
    "            all_preds.append(\" \".join(decoded_pred))\n",
    "            all_refs.append(\" \".join(decoded_ref))\n",
    "\n",
    "# √âvaluation\n",
    "print(f\"WER : {wer(all_refs, all_preds):.4f}\")\n",
    "print(f\"CER : {cer(all_refs, all_preds):.4f}\")\n",
    "print(f\"SER : {sum(p != r for p, r in zip(all_preds, all_refs)) / len(all_preds):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a689db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiwer import wer, cer\n",
    "\n",
    "# Charger le meilleur mod√®le\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_refs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for mel, target, input_lens, target_lens in test_loader:\n",
    "        mel = mel.to(device)\n",
    "        output = model(mel).log_softmax(2)\n",
    "        pred = output.argmax(2)  # [B, T]\n",
    "        pred = pred.cpu()\n",
    "\n",
    "        # target est concat√©n√©, on utilise un pointeur\n",
    "        start = 0\n",
    "        for i in range(len(mel)):\n",
    "            pred_seq = pred[i][:input_lens[i]].tolist()\n",
    "            decoded_pred = [vocab_inv.get(p, \"\") for p in pred_seq if p != 0]\n",
    "\n",
    "            t_len = target_lens[i]\n",
    "            ref_seq = target[start:start + t_len]\n",
    "            decoded_ref = [vocab_inv.get(t.item(), \"\") for t in ref_seq]\n",
    "\n",
    "            all_preds.append(\" \".join(decoded_pred))\n",
    "            all_refs.append(\" \".join(decoded_ref))\n",
    "\n",
    "            start += t_len\n",
    "\n",
    "# √âvaluation\n",
    "print(f\"WER : {wer(all_refs, all_preds):.4f}\")\n",
    "print(f\"CER : {cer(all_refs, all_preds):.4f}\")\n",
    "print(f\"SER : {sum(p != r for p, r in zip(all_preds, all_refs)) / len(all_preds):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783834be",
   "metadata": {},
   "source": [
    "# 10. Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f4eab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptation du YembaSataset et collate_fn pour qu'iol retourne aussi les chemins des fichiers\n",
    "\n",
    "class YembaDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        mel = extract_melspectrogram(row[\"audio_path\"])\n",
    "        label = torch.tensor(row[\"encoded\"], dtype=torch.long)\n",
    "        return mel, label, row[\"audio_path\"]\n",
    "\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    mels, labels, paths = zip(*batch)\n",
    "    input_lengths = [mel.shape[0] for mel in mels]\n",
    "    label_lengths = [len(label) for label in labels]\n",
    "\n",
    "    mels_padded = nn.utils.rnn.pad_sequence(mels, batch_first=True)  # [B, T, F]\n",
    "    labels_cat = torch.cat(labels)\n",
    "\n",
    "    return mels_padded, labels_cat, torch.tensor(input_lengths), torch.tensor(label_lengths), list(paths)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    YembaDataset(test), \n",
    "    batch_size=8, \n",
    "    collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aae9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# √âvaluation + collecte des r√©sultats avec noms de fichiers\n",
    "results = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for mel, target, input_lens, target_lens, paths in test_loader:\n",
    "        mel = mel.to(device)\n",
    "        output = model(mel).log_softmax(2)\n",
    "        pred = output.argmax(2).cpu()\n",
    "\n",
    "        start = 0\n",
    "        for i in range(len(mel)):\n",
    "            pred_seq = pred[i][:input_lens[i]].tolist()\n",
    "            decoded_pred = [vocab_inv.get(p, \"\") for p in pred_seq if p != 0]\n",
    "\n",
    "            t_len = target_lens[i]\n",
    "            ref_seq = target[start:start + t_len]\n",
    "            decoded_ref = [vocab_inv.get(t.item(), \"\") for t in ref_seq]\n",
    "\n",
    "            results.append({\n",
    "                \"fichier_audio\": paths[i],\n",
    "                \"prediction\": \" \".join(decoded_pred),\n",
    "                \"reference\": \" \".join(decoded_ref)\n",
    "            })\n",
    "\n",
    "            start += t_len\n",
    "\n",
    "# Cr√©ation du DataFrame\n",
    "df_resultats = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d2eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resultats.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4c96fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_erreurs(predictions, references):\n",
    "    for ref, pred in zip(references, predictions):\n",
    "        if ref != pred:\n",
    "            print(f\"\\nRef : {ref}\\nPred: {pred}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f261a101",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (ref, pred) in enumerate(zip(all_refs, all_preds)):\n",
    "    if ref != pred:\n",
    "        print(f\"\\nüî¥ Sample {i}\")\n",
    "        print(f\"R√©f√©rence : {ref}\")\n",
    "        print(f\"Pr√©dit     : {pred}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f83444",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9680e19d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44937ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a90db2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
