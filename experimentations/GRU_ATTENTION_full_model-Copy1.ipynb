{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6033a233",
   "metadata": {},
   "source": [
    "# 1. Préparation des données, analyses et extraction des transcriptions\n",
    "\n",
    "## 1.1. Preparations des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25d810b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torchaudio.transforms as T\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de255ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les transcriptions\n",
    "df = pd.read_excel(\"C:/Users/Christian/Desktop/YembaTones/dataset2.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "582878a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WordId</th>\n",
       "      <th>Yemba</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>GroupeId</th>\n",
       "      <th>Statement</th>\n",
       "      <th>Syllabe 1</th>\n",
       "      <th>Tone 1</th>\n",
       "      <th>Syllabe 2</th>\n",
       "      <th>Tone 2</th>\n",
       "      <th>Syllabe 3</th>\n",
       "      <th>Tone 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3779</th>\n",
       "      <td>3780</td>\n",
       "      <td>lezēn</td>\n",
       "      <td>11</td>\n",
       "      <td>146</td>\n",
       "      <td>2</td>\n",
       "      <td>le</td>\n",
       "      <td>bas</td>\n",
       "      <td>zēn</td>\n",
       "      <td>moyen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3780</th>\n",
       "      <td>3781</td>\n",
       "      <td>nzeŋ</td>\n",
       "      <td>11</td>\n",
       "      <td>147</td>\n",
       "      <td>1</td>\n",
       "      <td>n</td>\n",
       "      <td>bas</td>\n",
       "      <td>zeŋ</td>\n",
       "      <td>bas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3781</th>\n",
       "      <td>3782</td>\n",
       "      <td>nzéŋ</td>\n",
       "      <td>11</td>\n",
       "      <td>147</td>\n",
       "      <td>2</td>\n",
       "      <td>n</td>\n",
       "      <td>bas</td>\n",
       "      <td>zéŋ</td>\n",
       "      <td>haut</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3782</th>\n",
       "      <td>3783</td>\n",
       "      <td>nzɔ̄ŋ</td>\n",
       "      <td>11</td>\n",
       "      <td>148</td>\n",
       "      <td>1</td>\n",
       "      <td>n</td>\n",
       "      <td>bas</td>\n",
       "      <td>zɔ̄ŋ</td>\n",
       "      <td>moyen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3783</th>\n",
       "      <td>3784</td>\n",
       "      <td>nzɔŋ</td>\n",
       "      <td>11</td>\n",
       "      <td>148</td>\n",
       "      <td>2</td>\n",
       "      <td>n</td>\n",
       "      <td>bas</td>\n",
       "      <td>zɔŋ</td>\n",
       "      <td>bas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      WordId   Yemba  Speaker  GroupeId  Statement Syllabe 1 Tone 1 Syllabe 2  \\\n",
       "3779    3780  lezēn       11       146          2        le    bas      zēn   \n",
       "3780    3781    nzeŋ       11       147          1         n    bas       zeŋ   \n",
       "3781    3782   nzéŋ       11       147          2         n    bas      zéŋ   \n",
       "3782    3783   nzɔ̄ŋ       11       148          1         n    bas      zɔ̄ŋ   \n",
       "3783    3784    nzɔŋ       11       148          2         n    bas       zɔŋ   \n",
       "\n",
       "     Tone 2 Syllabe 3 Tone 3  \n",
       "3779  moyen       NaN    NaN  \n",
       "3780    bas       NaN    NaN  \n",
       "3781   haut       NaN    NaN  \n",
       "3782  moyen       NaN    NaN  \n",
       "3783    bas       NaN    NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5c6996b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordId          0\n",
      "Yemba           0\n",
      "Speaker         0\n",
      "GroupeId        0\n",
      "Statement       0\n",
      "Syllabe 1       0\n",
      "Tone 1          0\n",
      "Syllabe 2      22\n",
      "Tone 2         22\n",
      "Syllabe 3    3762\n",
      "Tone 3       3762\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c09d225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3784\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83225fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression définitive des colonnes Syllabe 3 et Tone 3\n",
    "df.drop(columns=[\"Syllabe 3\", \"Tone 3\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc0548a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remplacement des valeurs manquates\n",
    "df[\"Syllabe 2\"] = df[\"Syllabe 2\"].fillna(\"∅\")\n",
    "df[\"Tone 2\"]    = df[\"Tone 2\"].fillna(\"∅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25d2a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer les lignes où Syllabe 2 OU Tone 2 sont manquants ou égaux à \"∅\"\n",
    "df = df[~((df[\"Syllabe 2\"] == \"∅\") | (df[\"Tone 2\"] == \"∅\"))]\n",
    "df = df.reset_index(drop=True)  # Reindexer proprement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69570c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Génère les chemins des fichiers audio associés\n",
    "def get_audio_path(row):\n",
    "    return f\"C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_{row['Speaker']}/group_{row['GroupeId']}/spkr_{row['Speaker']}_group_{row['GroupeId']}_statement_{int(row['Statement'])}.wav\"\n",
    "\n",
    "df[\"audio_path\"] = df.apply(get_audio_path, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bec0cf1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WordId</th>\n",
       "      <th>Yemba</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>GroupeId</th>\n",
       "      <th>Statement</th>\n",
       "      <th>Syllabe 1</th>\n",
       "      <th>Tone 1</th>\n",
       "      <th>Syllabe 2</th>\n",
       "      <th>Tone 2</th>\n",
       "      <th>audio_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3757</th>\n",
       "      <td>3780</td>\n",
       "      <td>lezēn</td>\n",
       "      <td>11</td>\n",
       "      <td>146</td>\n",
       "      <td>2</td>\n",
       "      <td>le</td>\n",
       "      <td>bas</td>\n",
       "      <td>zēn</td>\n",
       "      <td>moyen</td>\n",
       "      <td>C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_11/group_146/spkr_11_group_146_statement_2.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3758</th>\n",
       "      <td>3781</td>\n",
       "      <td>nzeŋ</td>\n",
       "      <td>11</td>\n",
       "      <td>147</td>\n",
       "      <td>1</td>\n",
       "      <td>n</td>\n",
       "      <td>bas</td>\n",
       "      <td>zeŋ</td>\n",
       "      <td>bas</td>\n",
       "      <td>C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_11/group_147/spkr_11_group_147_statement_1.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3759</th>\n",
       "      <td>3782</td>\n",
       "      <td>nzéŋ</td>\n",
       "      <td>11</td>\n",
       "      <td>147</td>\n",
       "      <td>2</td>\n",
       "      <td>n</td>\n",
       "      <td>bas</td>\n",
       "      <td>zéŋ</td>\n",
       "      <td>haut</td>\n",
       "      <td>C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_11/group_147/spkr_11_group_147_statement_2.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3760</th>\n",
       "      <td>3783</td>\n",
       "      <td>nzɔ̄ŋ</td>\n",
       "      <td>11</td>\n",
       "      <td>148</td>\n",
       "      <td>1</td>\n",
       "      <td>n</td>\n",
       "      <td>bas</td>\n",
       "      <td>zɔ̄ŋ</td>\n",
       "      <td>moyen</td>\n",
       "      <td>C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_11/group_148/spkr_11_group_148_statement_1.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3761</th>\n",
       "      <td>3784</td>\n",
       "      <td>nzɔŋ</td>\n",
       "      <td>11</td>\n",
       "      <td>148</td>\n",
       "      <td>2</td>\n",
       "      <td>n</td>\n",
       "      <td>bas</td>\n",
       "      <td>zɔŋ</td>\n",
       "      <td>bas</td>\n",
       "      <td>C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_11/group_148/spkr_11_group_148_statement_2.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      WordId   Yemba  Speaker  GroupeId  Statement Syllabe 1 Tone 1 Syllabe 2  \\\n",
       "3757    3780  lezēn       11       146          2        le    bas      zēn   \n",
       "3758    3781    nzeŋ       11       147          1         n    bas       zeŋ   \n",
       "3759    3782   nzéŋ       11       147          2         n    bas      zéŋ   \n",
       "3760    3783   nzɔ̄ŋ       11       148          1         n    bas      zɔ̄ŋ   \n",
       "3761    3784    nzɔŋ       11       148          2         n    bas       zɔŋ   \n",
       "\n",
       "     Tone 2  \\\n",
       "3757  moyen   \n",
       "3758    bas   \n",
       "3759   haut   \n",
       "3760  moyen   \n",
       "3761    bas   \n",
       "\n",
       "                                                                                                                                                                                                   audio_path  \n",
       "3757  C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_11/group_146/spkr_11_group_146_statement_2.wav  \n",
       "3758  C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_11/group_147/spkr_11_group_147_statement_1.wav  \n",
       "3759  C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_11/group_147/spkr_11_group_147_statement_2.wav  \n",
       "3760  C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_11/group_148/spkr_11_group_148_statement_1.wav  \n",
       "3761  C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_11/group_148/spkr_11_group_148_statement_2.wav  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33662dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combiner les syllabes avec tons pour créer une transcription syllabique\n",
    "def combine_syllables(row):\n",
    "    syllables = []\n",
    "    for i in range(1, 4):\n",
    "        syll = row.get(f\"Syllabe {i}\")\n",
    "        tone = row.get(f\"Tone {i}\")\n",
    "        if pd.notnull(syll) and pd.notnull(tone):\n",
    "            syllables.append(f\"{syll}|{tone}\")\n",
    "    return \" \".join(syllables)\n",
    "\n",
    "df[\"syllable_transcript\"] = df.apply(combine_syllables, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37d722b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WordId</th>\n",
       "      <th>Yemba</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>GroupeId</th>\n",
       "      <th>Statement</th>\n",
       "      <th>Syllabe 1</th>\n",
       "      <th>Tone 1</th>\n",
       "      <th>Syllabe 2</th>\n",
       "      <th>Tone 2</th>\n",
       "      <th>audio_path</th>\n",
       "      <th>syllable_transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Apa</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>bas</td>\n",
       "      <td>pa</td>\n",
       "      <td>bas</td>\n",
       "      <td>C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_1/group_1/spkr_1_group_1_statement_1.wav</td>\n",
       "      <td>a|bas pa|bas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Apā</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>a</td>\n",
       "      <td>bas</td>\n",
       "      <td>pā</td>\n",
       "      <td>moyen</td>\n",
       "      <td>C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_1/group_1/spkr_1_group_1_statement_2.wav</td>\n",
       "      <td>a|bas pā|moyen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Apá</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>a</td>\n",
       "      <td>bas</td>\n",
       "      <td>pá</td>\n",
       "      <td>haut</td>\n",
       "      <td>C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_1/group_1/spkr_1_group_1_statement_3.wav</td>\n",
       "      <td>a|bas pá|haut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Api</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>bas</td>\n",
       "      <td>pi</td>\n",
       "      <td>bas</td>\n",
       "      <td>C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_1/group_2/spkr_1_group_2_statement_1.wav</td>\n",
       "      <td>a|bas pi|bas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Apī</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>a</td>\n",
       "      <td>bas</td>\n",
       "      <td>pī</td>\n",
       "      <td>moyen</td>\n",
       "      <td>C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_1/group_2/spkr_1_group_2_statement_2.wav</td>\n",
       "      <td>a|bas pī|moyen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   WordId Yemba  Speaker  GroupeId  Statement Syllabe 1 Tone 1 Syllabe 2  \\\n",
       "0       1   Apa        1         1          1         a    bas        pa   \n",
       "1       2  Apā        1         1          2         a    bas       pā   \n",
       "2       3  Apá        1         1          3         a    bas       pá   \n",
       "3       4   Api        1         2          1         a    bas        pi   \n",
       "4       5  Apī        1         2          2         a    bas       pī   \n",
       "\n",
       "   Tone 2  \\\n",
       "0     bas   \n",
       "1   moyen   \n",
       "2    haut   \n",
       "3     bas   \n",
       "4  moyen    \n",
       "\n",
       "                                                                                                                                                                                          audio_path  \\\n",
       "0  C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_1/group_1/spkr_1_group_1_statement_1.wav   \n",
       "1  C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_1/group_1/spkr_1_group_1_statement_2.wav   \n",
       "2  C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_1/group_1/spkr_1_group_1_statement_3.wav   \n",
       "3  C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_1/group_2/spkr_1_group_2_statement_1.wav   \n",
       "4  C:/Users/Christian/Desktop/YembaTones/YembaTones An Annotated Dataset for Tonal and Syllabic Analysis of the Yemba Language/Yemba_Dataset/audios/speaker_1/group_2/spkr_1_group_2_statement_2.wav   \n",
       "\n",
       "  syllable_transcript  \n",
       "0        a|bas pa|bas  \n",
       "1     a|bas pā|moyen  \n",
       "2      a|bas pá|haut  \n",
       "3        a|bas pi|bas  \n",
       "4    a|bas pī|moyen   "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0613d02",
   "metadata": {},
   "source": [
    "## 1.2. Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3632b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WordId</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>GroupeId</th>\n",
       "      <th>Statement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3762.000000</td>\n",
       "      <td>3762.000000</td>\n",
       "      <td>3762.000000</td>\n",
       "      <td>3762.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1892.307018</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>71.409357</td>\n",
       "      <td>1.730994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1092.515348</td>\n",
       "      <td>3.162698</td>\n",
       "      <td>42.952047</td>\n",
       "      <td>0.743985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>947.250000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1891.500000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>68.500000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2837.750000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3784.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>149.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            WordId      Speaker     GroupeId    Statement\n",
       "count  3762.000000  3762.000000  3762.000000  3762.000000\n",
       "mean   1892.307018     6.000000    71.409357     1.730994\n",
       "std    1092.515348     3.162698    42.952047     0.743985\n",
       "min       1.000000     1.000000     1.000000     1.000000\n",
       "25%     947.250000     3.000000    33.000000     1.000000\n",
       "50%    1891.500000     6.000000    68.500000     2.000000\n",
       "75%    2837.750000     9.000000   109.000000     2.000000\n",
       "max    3784.000000    11.000000   149.000000     4.000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13cab3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 syllabes les plus fréquentes :\n",
      "n      1287\n",
      "Le      924\n",
      "m       462\n",
      "a       440\n",
      "ŋ       176\n",
      "N       143\n",
      "le       99\n",
      "Me       66\n",
      "gap      44\n",
      "tā      33\n",
      "dtype: int64\n",
      "\n",
      "Nombre total de syllabes uniques : 301\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Compter toutes les syllabes\n",
    "syllabes = pd.concat([df['Syllabe 1'], df['Syllabe 2']])\n",
    "syllabe_counts = syllabes.value_counts()\n",
    "\n",
    "# Afficher les 10 syllabes les plus fréquentes\n",
    "print(\"Top 10 syllabes les plus fréquentes :\")\n",
    "print(syllabe_counts.head(10))\n",
    "\n",
    "# Afficher le nombre total de syllabes uniques\n",
    "print(f\"\\nNombre total de syllabes uniques : {syllabe_counts.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e830e88",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(syllabe_counts.head(20))\n",
    "\n",
    "syllabe_counts.head(20).plot(kind='bar', figsize=(10, 4), title='Top 20 syllabes les plus fréquentes')\n",
    "plt.ylabel(\"Fréquence\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48f86d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution nettoyée des tons dans Tone 1 :\n",
      "bas   : 2651\n",
      "haut  : 1111\n",
      "moyen : 0\n",
      "\n",
      "Distribution nettoyée des tons dans Tone 2 :\n",
      "bas   : 1287\n",
      "haut  : 990\n",
      "moyen : 1485\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def normalize_tone(t):\n",
    "    if isinstance(t, str):\n",
    "        return t.strip().lower()\n",
    "    return t\n",
    "\n",
    "# Normalisation des deux colonnes\n",
    "df['Tone 1 norm'] = df['Tone 1'].apply(normalize_tone)\n",
    "df['Tone 2 norm'] = df['Tone 2'].apply(normalize_tone)\n",
    "\n",
    "# Comptage par colonne\n",
    "tone1_dist = Counter(df['Tone 1 norm'])\n",
    "tone2_dist = Counter(df['Tone 2 norm'])\n",
    "\n",
    "# Affichage des distributions\n",
    "print(\"Distribution nettoyée des tons dans Tone 1 :\")\n",
    "for k in ['bas', 'haut', 'moyen']:\n",
    "    print(f\"{k:<6}: {tone1_dist.get(k, 0)}\")\n",
    "\n",
    "print(\"\\nDistribution nettoyée des tons dans Tone 2 :\")\n",
    "for k in ['bas', 'haut', 'moyen']:\n",
    "    print(f\"{k:<6}: {tone2_dist.get(k, 0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b781383",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fonction de normalisation des tons\n",
    "def normalize_tone(t):\n",
    "    if isinstance(t, str):\n",
    "        return t.strip().lower()\n",
    "    return t\n",
    "\n",
    "# Normalisation des deux colonnes\n",
    "df['Tone 1 norm'] = df['Tone 1'].apply(normalize_tone)\n",
    "df['Tone 2 norm'] = df['Tone 2'].apply(normalize_tone)\n",
    "\n",
    "# Comptage par colonne\n",
    "tone1_dist = Counter(df['Tone 1 norm'])\n",
    "tone2_dist = Counter(df['Tone 2 norm'])\n",
    "\n",
    "# Affichage texte\n",
    "print(\"Distribution nettoyée des tons dans Tone 1 :\")\n",
    "for k in ['bas', 'haut', 'moyen']:\n",
    "    print(f\"{k:<6}: {tone1_dist.get(k, 0)}\")\n",
    "\n",
    "print(\"\\nDistribution nettoyée des tons dans Tone 2 :\")\n",
    "for k in ['bas', 'haut', 'moyen']:\n",
    "    print(f\"{k:<6}: {tone2_dist.get(k, 0)}\")\n",
    "\n",
    "# Affichage graphique\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(['bas', 'haut', 'moyen'], [tone1_dist.get(k, 0) for k in ['bas', 'haut', 'moyen']])\n",
    "plt.title(\"Tone 1\")\n",
    "plt.ylabel(\"Fréquence\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(['bas', 'haut', 'moyen'], [tone2_dist.get(k, 0) for k in ['bas', 'haut', 'moyen']])\n",
    "plt.title(\"Tone 2\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0bb85b",
   "metadata": {},
   "source": [
    "## 1.3 Ponderation des classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6cdd8df",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'syllabe_counts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m syll_imbalance_ratio \u001b[38;5;241m=\u001b[39m syllabe_counts\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m/\u001b[39m syllabe_counts\u001b[38;5;241m.\u001b[39mmin()\n\u001b[0;32m      2\u001b[0m tone_imbalance_ratio \u001b[38;5;241m=\u001b[39m tone_counts\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m/\u001b[39m tone_counts\u001b[38;5;241m.\u001b[39mmin()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDéséquilibre syllabes : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msyll_imbalance_ratio\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'syllabe_counts' is not defined"
     ]
    }
   ],
   "source": [
    "syll_imbalance_ratio = syllabe_counts.max() / syllabe_counts.min()\n",
    "tone_imbalance_ratio = tone_counts.max() / tone_counts.min()\n",
    "\n",
    "print(f\"Déséquilibre syllabes : {syll_imbalance_ratio:.2f}\")\n",
    "print(f\"Déséquilibre tons : {tone_imbalance_ratio:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a370dcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41589e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# --- 1. Syllabe 1 ---\n",
    "syll1_classes = sorted(df['Syllabe 1'].dropna().unique())\n",
    "syll1_weights = compute_class_weight(class_weight='balanced',\n",
    "                                     classes=syll1_classes,\n",
    "                                     y=df['Syllabe 1'].dropna())\n",
    "syll1_weights_tensor = torch.tensor(syll1_weights, dtype=torch.float32)\n",
    "\n",
    "# --- 2. Tone 1 norm ---\n",
    "tone1_classes = sorted(df['Tone 1 norm'].dropna().unique())\n",
    "tone1_weights = compute_class_weight(class_weight='balanced',\n",
    "                                     classes=tone1_classes,\n",
    "                                     y=df['Tone 1 norm'].dropna())\n",
    "tone1_weights_tensor = torch.tensor(tone1_weights, dtype=torch.float32)\n",
    "\n",
    "# --- 3. Syllabe 2 ---\n",
    "syll2_classes = sorted(df['Syllabe 2'].dropna().unique())\n",
    "syll2_weights = compute_class_weight(class_weight='balanced',\n",
    "                                     classes=syll2_classes,\n",
    "                                     y=df['Syllabe 2'].dropna())\n",
    "syll2_weights_tensor = torch.tensor(syll2_weights, dtype=torch.float32)\n",
    "\n",
    "# --- 4. Tone 2 norm ---\n",
    "tone2_classes = sorted(df['Tone 2 norm'].dropna().unique())\n",
    "tone2_weights = compute_class_weight(class_weight='balanced',\n",
    "                                     classes=tone2_classes,\n",
    "                                     y=df['Tone 2 norm'].dropna())\n",
    "tone2_weights_tensor = torch.tensor(tone2_weights, dtype=torch.float32)\n",
    "\n",
    "syll1_loss_fn = torch.nn.CrossEntropyLoss(weight=syll1_weights_tensor)\n",
    "tone1_loss_fn = torch.nn.CrossEntropyLoss(weight=tone1_weights_tensor)\n",
    "syll2_loss_fn = torch.nn.CrossEntropyLoss(weight=syll2_weights_tensor)\n",
    "tone2_loss_fn = torch.nn.CrossEntropyLoss(weight=tone2_weights_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88be3e30",
   "metadata": {},
   "source": [
    "# 2. Data augmentation et Extraction des caractéristiques audio avec MelSpectrogram\n",
    "\n",
    "## 2.1. Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70aa8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import random\n",
    "import torch\n",
    "\n",
    "def augment_waveform(waveform, sample_rate=16000):\n",
    "    \"\"\"Applique des transformations audio de data augmentation.\"\"\"\n",
    "    \n",
    "    # 1. Ajouter bruit blanc (SNR ~20 dB)\n",
    "    if random.random() < 0.5:\n",
    "        noise = torch.randn_like(waveform) * 0.005\n",
    "        waveform = waveform + noise\n",
    "\n",
    "    # 2. Time shifting (décalage temporel aléatoire)\n",
    "    if random.random() < 0.5:\n",
    "        shift = int(random.uniform(-0.1, 0.1) * waveform.size(1))  # ±10%\n",
    "        waveform = torch.roll(waveform, shifts=shift, dims=1)\n",
    "\n",
    "    # 3. Pitch shift\n",
    "    if random.random() < 0.3:\n",
    "        semitone_shift = random.choice([-2, -1, 1, 2])\n",
    "        pitch_shift = T.PitchShift(sample_rate, n_steps=semitone_shift)\n",
    "        waveform = pitch_shift(waveform)\n",
    "\n",
    "    # 4. Time stretch (entre 0.8x et 1.2x)\n",
    "    if random.random() < 0.3:\n",
    "        stretch_rate = random.uniform(0.8, 1.2)\n",
    "        stretch = T.TimeStretch()\n",
    "        spec = T.Spectrogram()(waveform)\n",
    "        spec_stretched = stretch(spec, stretch_rate)\n",
    "        waveform = torchaudio.functional.istft(spec_stretched, n_fft=400)\n",
    "        waveform = waveform.unsqueeze(0)  # [1, T]\n",
    "\n",
    "    return waveform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b43854",
   "metadata": {},
   "source": [
    "## 2.2. Melspectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d963b31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "\n",
    "def extract_melspectrogram(file_path, sample_rate=16000, n_mels=80):\n",
    "    try:\n",
    "        waveform, sr = torchaudio.load(file_path)\n",
    "\n",
    "        # Force mono (si 2 canaux, on moyenne)\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "        # Resample si nécessaire\n",
    "        if sr != sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "\n",
    "        # Transformer Mel spectrogramme\n",
    "        mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=1024,\n",
    "            hop_length=256,\n",
    "            n_mels=n_mels\n",
    "        )\n",
    "        mel_spec = mel_transform(waveform)\n",
    "\n",
    "        # Convertir en dB\n",
    "        mel_spec = torchaudio.transforms.AmplitudeToDB()(mel_spec)\n",
    "\n",
    "        # [1, F, T] → [T, F]\n",
    "        mel_spec = mel_spec.squeeze(0).transpose(0, 1)\n",
    "\n",
    "        # Vérifier la forme finale\n",
    "        if mel_spec.shape[1] != n_mels:\n",
    "            raise ValueError(f\"Mel spectrogram with invalid feature size: {mel_spec.shape}\")\n",
    "\n",
    "        return mel_spec\n",
    "\n",
    "    except Exception as e:\n",
    "        #print(f\"Erreur lors du traitement de {file_path}: {e}\")\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec42282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_melspectrogram_augmented(file_path, sample_rate=16000, n_mels=80):\n",
    "    try:\n",
    "        waveform, sr = torchaudio.load(file_path)\n",
    "\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "        if sr != sample_rate:\n",
    "            resampler = T.Resample(sr, sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "\n",
    "        # --- AJOUT DES AUGMENTATIONS ---\n",
    "        waveform = augment_waveform(waveform, sample_rate=sample_rate)\n",
    "\n",
    "        mel_transform = T.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=1024,\n",
    "            hop_length=256,\n",
    "            n_mels=n_mels\n",
    "        )\n",
    "        mel_spec = mel_transform(waveform)\n",
    "        mel_spec = T.AmplitudeToDB()(mel_spec)\n",
    "        mel_spec = mel_spec.squeeze(0).transpose(0, 1)\n",
    "\n",
    "        if mel_spec.shape[1] != n_mels:\n",
    "            raise ValueError(f\"Mel spectrogram with invalid feature size: {mel_spec.shape}\")\n",
    "\n",
    "        return mel_spec\n",
    "    except Exception as e:\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2661240",
   "metadata": {},
   "source": [
    "# 3. Tokenisation syllabique et construction du vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07f6050",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Créer un vocabulaire syllabique\n",
    "syllables = df[\"syllable_transcript\"].str.split().sum()\n",
    "syllable_counts = Counter(syllables)\n",
    "vocab = {s: i + 1 for i, s in enumerate(sorted(syllable_counts))}\n",
    "vocab[\"<BLANK>\"] = 0  # pour CTC\n",
    "\n",
    "# Encodage des transcriptions\n",
    "def encode_transcript(syllable_transcript):\n",
    "    return [vocab[s] for s in syllable_transcript.split()]\n",
    "\n",
    "df[\"encoded\"] = df[\"syllable_transcript\"].map(encode_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e3145e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac68f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"audio_path\"].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8ebd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"exists\"] = df[\"audio_path\"].apply(lambda p: os.path.exists(p))\n",
    "df = df[df[\"exists\"]]\n",
    "\n",
    "missing = df[~df[\"exists\"]]\n",
    "print(\"Fichiers manquants :\", len(missing))\n",
    "print(missing[[\"audio_path\", \"Speaker\", \"GroupeId\", \"Statement\"]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf70196b",
   "metadata": {},
   "source": [
    "# 4. Split en train/valid/test (80/10/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b993bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val, test = train_test_split(test, test_size=0.5, random_state=42)\n",
    "\n",
    "print(len(train), \"train\")\n",
    "print(len(val), \"val\")\n",
    "print(len(test), \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86635be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2825cf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.rename(columns={\"audio_path\": \"path\"})\n",
    "val = val.rename(columns={\"audio_path\": \"path\"})\n",
    "test = test.rename(columns={\"audio_path\": \"path\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dda2a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifie que 'path' et 'encoded' sont bien dans chaque ligne\n",
    "train = train[train['path'].notnull() & train['encoded'].notnull()]\n",
    "val = val[val['path'].notnull() & val['encoded'].notnull()]\n",
    "test = test[test['path'].notnull() & test['encoded'].notnull()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f3e977",
   "metadata": {},
   "source": [
    "# 5. Dataset & DataLoader PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35404ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class YembaDataset(Dataset):\n",
    "    def __init__(self, dataframe, processor, vocab):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe: DataFrame contenant les colonnes 'path' et 'encoded'\n",
    "            processor: fonction pour extraire un Mel spectrogramme à partir d'un fichier audio\n",
    "            vocab: dictionnaire {syllabe: index}\n",
    "        \"\"\"\n",
    "        self.data = dataframe\n",
    "        self.processor = processor\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            row = self.data.iloc[idx]\n",
    "            audio_path = row[\"path\"]\n",
    "            target_seq = row[\"encoded\"]\n",
    "\n",
    "            # 1. Extraire les features [T, F]\n",
    "            mel_spec = self.processor(audio_path)\n",
    "\n",
    "            # 2. Convertir les cibles en Tensor\n",
    "            target_tensor = torch.tensor(target_seq, dtype=torch.long)\n",
    "\n",
    "            return mel_spec, target_tensor\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[!] Erreur à l’index {idx} : {e}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f9abcc",
   "metadata": {},
   "source": [
    "# 6. Collate function pour padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a50feef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "PAD_IDX = vocab.get(PAD_TOKEN, 0)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function pour DataLoader :\n",
    "    batch : list de tuples (mel_spec, target_tensor)\n",
    "            - mel_spec: Tensor de forme [T, F]\n",
    "            - target_tensor: Tensor de forme [L]\n",
    "    \"\"\"\n",
    "    # Supprimer les entrées invalides\n",
    "    batch = [sample for sample in batch if sample is not None]\n",
    "    \n",
    "    for i, item in enumerate(batch):\n",
    "        if not isinstance(item, tuple):\n",
    "            print(f\"[!] Élément invalide à l’index {i} : {item}\")\n",
    "        elif not isinstance(item[0], torch.Tensor):\n",
    "            print(f\"[!] Mel_spec invalide à l’index {i} : {item[0]}\")\n",
    "        elif not isinstance(item[1], torch.Tensor):\n",
    "            print(f\"[!] Target invalide à l’index {i} : {item[1]}\")\n",
    "\n",
    "    # Décompacter les mel_specs et cibles\n",
    "    mels, targets = zip(*batch)\n",
    "\n",
    "    # Pad des mel spectrogrammes (batch_first = True → [B, Tmax, F])\n",
    "    mels_padded = rnn_utils.pad_sequence(mels, batch_first=True)\n",
    "    input_lengths = torch.tensor([mel.shape[0] for mel in mels], dtype=torch.long)\n",
    "\n",
    "    # Pad des séquences cibles\n",
    "    targets_padded = rnn_utils.pad_sequence(targets, batch_first=True, padding_value=PAD_IDX)\n",
    "    target_lengths = torch.tensor([t.shape[0] for t in targets], dtype=torch.long)\n",
    "\n",
    "    return mels_padded, targets_padded, input_lengths, target_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966cc032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = YembaDataset(train, processor=extract_melspectrogram_augmented, vocab=vocab)\n",
    "val_dataset = YembaDataset(val, processor=extract_melspectrogram, vocab=vocab)\n",
    "test_dataset = YembaDataset(test, processor=extract_melspectrogram, vocab=vocab)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26745023",
   "metadata": {},
   "source": [
    "# 7. Modèle GRU+ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813437bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GRUEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers=num_layers,\n",
    "                          batch_first=True, bidirectional=True, dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs, hidden = self.gru(x)  # outputs: [B, T, 2*H], hidden: [2L, B, H]\n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hidden_dim * 3, 1)\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_outputs):\n",
    "        # decoder_hidden: [B, H]\n",
    "        # encoder_outputs: [B, T, 2*H]\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "        decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, seq_len, 1)  # [B, T, H]\n",
    "        concat = torch.cat((decoder_hidden, encoder_outputs), dim=2)        # [B, T, 3H]\n",
    "        energy = self.attn(concat).squeeze(2)                               # [B, T]\n",
    "        attn_weights = F.softmax(energy, dim=1)                             # [B, T]\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)     # [B, 1, 2H]\n",
    "        return context.squeeze(1), attn_weights\n",
    "\n",
    "\n",
    "class GRUDecoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, num_layers=1, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(hidden_dim * 2, hidden_dim, num_layers=num_layers,\n",
    "                          batch_first=True, dropout=dropout)\n",
    "        self.out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, context_vector, hidden):\n",
    "        # context_vector: [B, 2H] → expand to [B, 1, 2H]\n",
    "        input_step = context_vector.unsqueeze(1)\n",
    "        output, hidden = self.gru(input_step, hidden)  # output: [B, 1, H]\n",
    "        prediction = self.out(output.squeeze(1))       # [B, vocab_size]\n",
    "        return prediction, hidden\n",
    "\n",
    "\n",
    "class GRUSeq2Seq(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, vocab_size, encoder_layers=2, decoder_layers=1):\n",
    "        super().__init__()\n",
    "        self.bridge = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.encoder = GRUEncoder(input_dim, hidden_dim, encoder_layers)\n",
    "        self.attention = Attention(hidden_dim)\n",
    "        self.decoder = GRUDecoder(hidden_dim, vocab_size, decoder_layers)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.encoder_layers = encoder_layers\n",
    "\n",
    "    def forward(self, x, max_len=100):\n",
    "        encoder_outputs, hidden = self.encoder(x)  # hidden: [2*num_layers, B, H]\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # [2*L, B, H] → [L, 2, B, H]\n",
    "        hidden = hidden.view(self.encoder_layers, 2, batch_size, self.hidden_dim)\n",
    "\n",
    "        # Concat direction=0 et direction=1 → [L, B, 2H]\n",
    "        hidden_cat = torch.cat((hidden[:, 0], hidden[:, 1]), dim=-1)\n",
    "\n",
    "        # Projeter en [L, B, H] via Linear (bridge)\n",
    "        if not hasattr(self, \"bridge\"):\n",
    "            self.bridge = nn.Linear(self.hidden_dim * 2, self.hidden_dim).to(hidden_cat.device)\n",
    "\n",
    "        hidden_proj = self.bridge(hidden_cat)  # [L, B, H]\n",
    "\n",
    "        # Si besoin, adapter au nombre de couches du décodeur\n",
    "        if self.encoder_layers < self.decoder.gru.num_layers:\n",
    "            # Répéter pour compléter\n",
    "            num_missing = self.decoder.gru.num_layers - self.encoder_layers\n",
    "            repeat_hidden = hidden_proj[-1:, :, :].repeat(num_missing, 1, 1)\n",
    "            hidden_proj = torch.cat([hidden_proj, repeat_hidden], dim=0)\n",
    "        elif self.encoder_layers > self.decoder.gru.num_layers:\n",
    "            hidden_proj = hidden_proj[:self.decoder.gru.num_layers]\n",
    "\n",
    "        hidden = hidden_proj  # [decoder_layers, B, H]\n",
    "\n",
    "        outputs = []\n",
    "        for _ in range(max_len):\n",
    "            context, attn_weights = self.attention(hidden[-1], encoder_outputs)  # [B, 2H]\n",
    "            prediction, hidden = self.decoder(context, hidden)  # hidden: [decoder_layers, B, H]\n",
    "            outputs.append(prediction)\n",
    "\n",
    "        outputs = torch.stack(outputs, dim=1)  # [B, max_len, vocab_size]\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce34d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from model import GRUSeq2Seq  # si défini dans model.py\n",
    "# ou bien directement : from ton_module import GRUSeq2Seq\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instanciation du modèle avec ton vocabulaire\n",
    "model = GRUSeq2Seq(input_dim=80, hidden_dim=256, vocab_size=len(vocab)).to(device)\n",
    "\n",
    "# Critère de perte adapté à une sortie par pas de temps\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "PAD_IDX = vocab[PAD_TOKEN] if PAD_TOKEN in vocab else 0  # ou choisis un autre index\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)  # définir PAD_IDX selon ton vocab\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Inverse vocab pour décodage (inchangé)\n",
    "vocab_inv = {v: k for k, v in vocab.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3988928f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"vocabulaire.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(vocab, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a94445",
   "metadata": {},
   "source": [
    "# 8. Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020eea6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CTCLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = GRUSeq2Seq(input_dim=80, hidden_dim=256, vocab_size=len(vocab),\n",
    "                   encoder_layers=2, decoder_layers=2).to(device)\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "PAD_IDX = vocab.get(PAD_TOKEN, 0)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for mel, target, input_lens, target_lens in tqdm(train_loader):\n",
    "        mel = mel.to(device)                 # [B, T, F]\n",
    "        target = target.to(device)           # [B, L]  (déjà pad)\n",
    "        \n",
    "        output = model(mel, max_len=target.shape[1])  # [B, L, vocab_size]\n",
    "        \n",
    "        # Reshape for CrossEntropy: [B*L, vocab_size] vs [B*L]\n",
    "        output = output.view(-1, output.size(2))  # [B*L, V]\n",
    "        target = target.view(-1)                  # [B*L]\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(train_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817f6e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jiwer\n",
    "import time\n",
    "\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "PAD_IDX = vocab.get(PAD_TOKEN, 0)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "patience_counter = 0\n",
    "num_epochs = 30\n",
    "train_losses, val_losses, wer_train, wer_val = [], [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\n--- Époque {epoch + 1} ---\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # === TRAIN ===\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for mel, target, input_lens, target_lens in tqdm(train_loader):\n",
    "        mel, target = mel.to(device), target.to(device)\n",
    "\n",
    "        output = model(mel, max_len=target.size(1))  # [B, L, vocab_size]\n",
    "        output = output.view(-1, output.size(-1))    # [B*L, vocab_size]\n",
    "        target = target.view(-1)                     # [B*L]\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f\"Perte entraînement : {avg_train_loss:.4f}\")\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # === VALID ===\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for mel, target, input_lens, target_lens in val_loader:\n",
    "            mel, target = mel.to(device), target.to(device)\n",
    "            output = model(mel, max_len=target.size(1))  # [B, L, vocab_size]\n",
    "            output = output.view(-1, output.size(-1))\n",
    "            target = target.view(-1)\n",
    "            loss = criterion(output, target)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Perte validation : {avg_val_loss:.4f}\")\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    # === Évaluer WER simplifié ===\n",
    "    def compute_wer(loader):\n",
    "        model.eval()\n",
    "        predictions, references = [], []\n",
    "        with torch.no_grad():\n",
    "            for mel, target, _, target_lens in loader:\n",
    "                mel = mel.to(device)\n",
    "                output = model(mel, max_len=target.size(1))  # [B, L, vocab_size]\n",
    "                pred_ids = output.argmax(dim=-1).cpu()       # [B, L]\n",
    "\n",
    "                for pred_seq, ref_seq in zip(pred_ids, target):\n",
    "                    pred_str = \" \".join([vocab_inv.get(idx.item(), \"\") for idx in pred_seq if idx != PAD_IDX])\n",
    "                    ref_str = \" \".join([vocab_inv.get(idx.item(), \"\") for idx in ref_seq if idx != PAD_IDX])\n",
    "                    predictions.append(pred_str.strip())\n",
    "                    references.append(ref_str.strip())\n",
    "\n",
    "        return jiwer.wer(references, predictions)\n",
    "\n",
    "    wer_t = compute_wer(train_loader)\n",
    "    wer_v = compute_wer(val_loader)\n",
    "    wer_train.append(wer_t)\n",
    "    wer_val.append(wer_v)\n",
    "    print(f\"WER entraînement : {wer_t:.4f}\")\n",
    "    print(f\"WER validation   : {wer_v:.4f}\")\n",
    "    print(f\"Temps écoulé : {time.time() - start_time:.2f}s\")\n",
    "\n",
    "    # === Early stopping ===\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"full_model_GRUSeq2Seq.pt\")\n",
    "        print(\"✅ Nouveau meilleur modèle sauvegardé\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"⛔ Arrêt anticipé (early stopping)\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5adc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "epochs = list(range(1, len(train_losses) + 1))\n",
    "\n",
    "# 📉 Courbe des pertes (Loss)\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_losses, label='Perte Entraînement')\n",
    "plt.plot(epochs, val_losses, label='Perte Validation')\n",
    "plt.title(\"Évolution de la Perte\")\n",
    "plt.xlabel(\"Époque\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 🧠 Courbe du Word Error Rate (WER)\n",
    "plt.figure()\n",
    "plt.plot(epochs, wer_train, label='WER Entraînement')\n",
    "plt.plot(epochs, wer_val, label='WER Validation')\n",
    "plt.title(\"Évolution du WER (Word Error Rate)\")\n",
    "plt.xlabel(\"Époque\")\n",
    "plt.ylabel(\"WER\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94200257",
   "metadata": {},
   "source": [
    "# 9.evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d459f5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification du contenu du test_loader\n",
    "for batch in test_loader:\n",
    "    if batch is None or len(batch) == 0:\n",
    "        print(\"❌ Batch vide ou invalide.\")\n",
    "        continue\n",
    "\n",
    "    if len(batch) != 4:\n",
    "        print(f\"❌ Format inattendu du batch : attendu 4 éléments (mel, target, input_lens, target_lens), obtenu {len(batch)}\")\n",
    "        continue\n",
    "\n",
    "    mel, target, input_lens, target_lens = batch\n",
    "\n",
    "    print(\"✅ Batch trouvé avec données valides :\")\n",
    "    print(f\"- mel: {mel.shape}\")            # [B, T, F]\n",
    "    print(f\"- target: {target.shape}\")      # [B, L]\n",
    "    print(f\"- input_lens: {input_lens}\")    # liste des longueurs réelles d’entrée\n",
    "    print(f\"- target_lens: {target_lens}\")  # liste des longueurs cibles\n",
    "\n",
    "    # Afficher un exemple brut\n",
    "    print(\"\\nExemple brut :\")\n",
    "    print(\"→ mel[0].shape :\", mel[0].shape)\n",
    "    print(\"→ target[0] :\", target[0])\n",
    "    print(\"→ input_lens[0] :\", input_lens[0])\n",
    "    print(\"→ target_lens[0] :\", target_lens[0])\n",
    "    break  # On ne regarde que le premier batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b27275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_seq2seq(model, loader, vocab_inv, pad_idx=0):\n",
    "    model.eval()\n",
    "    predictions, references = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for mel, target, input_lens, target_lens in loader:\n",
    "            if mel is None or target is None:\n",
    "                continue  # Sécurité\n",
    "\n",
    "            mel = mel.to(model.device if hasattr(model, \"device\") else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            target = target.to(mel.device)\n",
    "\n",
    "            output = model(mel, max_len=target.shape[1])  # [B, L, Vocab]\n",
    "            pred_tokens = output.argmax(dim=-1).cpu()     # [B, L]\n",
    "\n",
    "            for i in range(len(mel)):\n",
    "                # Prédiction\n",
    "                pred_seq = pred_tokens[i].tolist()\n",
    "                pred_seq = [p for p in pred_seq if p != pad_idx]\n",
    "                decoded_pred = [vocab_inv.get(p, \"\") for p in pred_seq]\n",
    "\n",
    "                # Référence\n",
    "                ref_seq = target[i][:target_lens[i]].tolist()\n",
    "                decoded_ref = [vocab_inv.get(t, \"\") for t in ref_seq]\n",
    "\n",
    "                predictions.append(\" \".join(decoded_pred))\n",
    "                references.append(\" \".join(decoded_ref))\n",
    "\n",
    "    # Sécurité : éviter la division par zéro\n",
    "    if len(references) == 0:\n",
    "        print(\"[⚠️] Aucune référence trouvée. Vérifie le contenu du test_loader.\")\n",
    "        return {\"WER\": None, \"CER\": None, \"SER\": None}\n",
    "\n",
    "    # Afficher un exemple pour vérification\n",
    "    print(\"\\n✅ Exemple de prédiction :\")\n",
    "    print(\"PRED :\", predictions[0])\n",
    "    print(\"REF  :\", references[0])\n",
    "\n",
    "    return {\n",
    "        \"WER\": wer(references, predictions),\n",
    "        \"CER\": cer(references, predictions),\n",
    "        \"SER\": sum([p != r for p, r in zip(predictions, references)]) / len(references)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e92c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_seq2seq(model, test_loader, vocab_inv, pad_idx=PAD_IDX)\n",
    "print(f\"WER : {results['WER']:.4f}\")\n",
    "print(f\"CER : {results['CER']:.4f}\")\n",
    "print(f\"SER : {results['SER']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88e360a",
   "metadata": {},
   "source": [
    "# 10. Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a53ff24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b14dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptation du YembaSataset et collate_fn pour qu'il retourne aussi les chemins des fichiers\n",
    "\n",
    "class YembaDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        mel = extract_melspectrogram(row[\"path\"])\n",
    "        label = torch.tensor(row[\"encoded\"], dtype=torch.long)\n",
    "        return mel, label, row[\"path\"]\n",
    "\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    mels, labels, paths = zip(*batch)\n",
    "    input_lengths = [mel.shape[0] for mel in mels]\n",
    "    label_lengths = [len(label) for label in labels]\n",
    "\n",
    "    mels_padded = nn.utils.rnn.pad_sequence(mels, batch_first=True)  # [B, T, F]\n",
    "    labels_cat = torch.cat(labels)\n",
    "\n",
    "    return mels_padded, labels_cat, torch.tensor(input_lengths), torch.tensor(label_lengths), list(paths)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    YembaDataset(test), \n",
    "    batch_size=8, \n",
    "    collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf655c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_prediction(pred_list):\n",
    "    cleaned = []\n",
    "    for token in pred_list:\n",
    "        if \"|\" in token:\n",
    "            base = token.split(\"|\")[0]\n",
    "        else:\n",
    "            base = token\n",
    "        if base.strip() == \"\" or base == \"∅\":\n",
    "            continue\n",
    "        cleaned.append(base)\n",
    "    return \"\".join(cleaned)\n",
    "\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b336eccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for mel, target, input_lens, target_lens, paths in test_loader:\n",
    "        mel = mel.to(device)\n",
    "        output = model(mel).log_softmax(2)\n",
    "        pred = output.argmax(2).cpu()\n",
    "\n",
    "        start = 0\n",
    "        for i in range(len(mel)):\n",
    "            pred_seq = pred[i][:input_lens[i]].tolist()\n",
    "            decoded_pred = [vocab_inv.get(p, \"\") for p in pred_seq if p != 0]\n",
    "\n",
    "            t_len = target_lens[i]\n",
    "            ref_seq = target[start:start + t_len]\n",
    "            decoded_ref = [vocab_inv.get(t.item(), \"\") for t in ref_seq]\n",
    "\n",
    "            results.append({\n",
    "                \"fichier_audio\": os.path.basename(paths[i]),\n",
    "                \"prediction\": clean_prediction(decoded_pred),\n",
    "                \"reference\": clean_prediction(decoded_ref)\n",
    "            })\n",
    "\n",
    "            start += t_len\n",
    "\n",
    "df_resultats = pd.DataFrame(results)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eef8f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_resultats))\n",
    "df_resultats.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106fd1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_correct = df_resultats[df_resultats[\"prediction\"] == df_resultats[\"reference\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12acdcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_correct))\n",
    "df_correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d0a4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = len(df_correct) / len(df_resultats)\n",
    "print(f\"Exact Match Accuracy : {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdde5c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
